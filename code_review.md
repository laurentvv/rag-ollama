Voici une analyse d√©taill√©e et des am√©liorations concr√®tes pour votre projet RAG-Ollama. J'ai identifi√© plusieurs axes d'am√©lioration structurale, de robustesse et de maintenabilit√©.

## 1. Consolidation de la Configuration (Priorit√© Haute)

**Probl√®me** : Le `config.yaml` n'est jamais charg√©. Les `yaml_conf = {}` sont des placeholders inop√©rants.

**Solution** : Cr√©ez un chargeur de config centralis√© :

```python
# src/rag_ollama/config.py
from dataclasses import dataclass, fields
from pathlib import Path
import yaml
from typing import Any, Dict, Optional

@dataclass
class RAGConfig:
    source_dir: Path = Path("./sources")
    processed_dir: Path = Path("./processed_md")
    db_path: Path = Path("./chroma_db")
    vision_model: str = "llama3.2-vision"
    pdf_model: str = "llama3.2-vision"
    llm_model: str = "gemma3:12b"
    embedding_model: str = "embeddinggemma:latest"

def load_config(config_path: Path = Path("config.yaml")) -> RAGConfig:
    """Charge la configuration depuis YAML avec fallback sur les valeurs par d√©faut."""
    defaults = {f.name: f.default for f in fields(RAGConfig) if f.default is not fields._MISSING_TYPE}
    
    if config_path.exists():
        with open(config_path, "r", encoding="utf-8") as f:
            yaml_data = yaml.safe_load(f) or {}
        
        # Fusionner avec les valeurs par d√©faut
        merged = {}
        for key, default_value in defaults.items():
            section_key = key.split('_')[0]  # e.g., "vision_model" -> "vision"
            section = yaml_data.get(section_key, {})
            merged[key] = section.get(key, default_value)
        
        return RAGConfig(**{k: v for k, v in merged.items() if k in defaults})
    
    return RAGConfig()

def save_config(config: RAGConfig, config_path: Path = Path("config.yaml")):
    """Sauvegarde la configuration dans YAML."""
    yaml_data = {
        "prepare": {
            "vision_model": config.vision_model,
            "pdf_model": config.pdf_model,
        },
        "chat": {
            "llm_model": config.llm_model,
            "embedding_model": config.embedding_model,
        }
    }
    with open(config_path, "w", encoding="utf-8") as f:
        yaml.dump(yaml_data, f, default_flow_style=False, indent=2)
```

**Mise √† jour de `prepare_documents.py`** :

```python
# src/rag_ollama/prepare_documents.py
# ...
def main():
    try:
        config = load_config()  # Charge le YAML
        
        parser = argparse.ArgumentParser(description="Pr√©traitement de documents pour RAG.")
        parser.add_argument("--input", "-i", type=Path, required=True)
        parser.add_argument("--output", "-o", type=Path, required=True)
        parser.add_argument("--vision-model", default=config.vision_model)
        parser.add_argument("--pdf-model", default=config.pdf_model)
        args = parser.parse_args()

        # Override config with CLI args
        config.source_dir = args.input.resolve()
        config.processed_dir = args.output.resolve()
        config.vision_model = args.vision_model
        config.pdf_model = args.pdf_model
        
        # Suite du code...
```

## 2. Refactorisation des V√©rifications Ollama (√âvite la duplication)

**Probl√®me** : Les v√©rifications Ollama sont r√©p√©t√©es dans plusieurs fichiers.

**Solution** : Cr√©ez un gestionnaire de mod√®les d√©di√© :

```python
# src/rag_ollama/utils/ollama_manager.py
import ollama
import time
from pathlib import Path
from .exceptions import OllamaError, ModelUnavailableError
from .logging import logger

class OllamaManager:
    def __init__(self, host: str = "http://localhost:11434"):
        self.client = ollama.Client(host=host, timeout=30)
    
    def check_connection(self):
        try:
            self.client.list()
        except Exception as e:
            raise OllamaError(f"Impossible de communiquer avec Ollama sur {self.client.host}: {e}")
    
    def check_model(self, model_name: str, auto_pull: bool = True):
        """V√©rifie si un mod√®le existe, le t√©l√©charge si n√©cessaire."""
        try:
            models = self.client.list()
            available = [m['model'] for m in models.get('models', [])]
            
            # V√©rification plus robuste (tag ou nom de base)
            model_base = model_name.split(':')[0]
            if not any(model_name == m or model_base in m for m in available):
                if auto_pull:
                    logger.info(f"Mod√®le {model_name} non trouv√©. Tentative de t√©l√©chargement...")
                    self.client.pull(model_name)
                    logger.info(f"‚úÖ Mod√®le {model_name} t√©l√©charg√©.")
                else:
                    raise ModelUnavailableError(f"Mod√®le '{model_name}' non trouv√©.")
        except Exception as e:
            if "pull" in str(e).lower():
                raise ModelUnavailableError(f"Mod√®le '{model_name}' non trouv√© et √©chec du t√©l√©chargement: {e}")
            raise OllamaError(f"Erreur lors de la v√©rification du mod√®le: {e}")
    
    def unload_model(self, model_name: str):
        """D√©charge un mod√®le de la m√©moire."""
        try:
            self.client.chat(model=model_name, messages=[], keep_alive=0)
            logger.debug(f"Mod√®le {model_name} d√©charg√©.")
            time.sleep(1)  # Pause pour lib√©ration m√©moire
        except Exception as e:
            logger.warning(f"Erreur lors du d√©chargement de {model_name}: {e}")
```

**Utilisation dans `prepare_documents.py`** :

```python
from .utils.ollama_manager import OllamaManager

def main():
    # ...
    manager = OllamaManager()
    manager.check_connection()
    manager.check_model(config.vision_model)
    manager.check_model(config.pdf_model)
    # ...
```

## 3. Am√©lioration des Processeurs (Robustesse & S√©curit√©)

**Probl√®me** : Ex√©cution de commandes shell sans validation des chemins.

**Solution** : Validez les ex√©cutables et les chemins :

```python
# src/rag_ollama/processors/pdf.py
from pathlib import Path
import shutil
from .base import DocumentProcessor
from ..config import RAGConfig
from ..utils.exceptions import ProcessingError

class PDFProcessor(DocumentProcessor):
    def can_process(self, file_path: Path) -> bool:
        return file_path.suffix.lower() == ".pdf"
    
    def _find_executable(self) -> Path:
        """Trouve l'ex√©cutable pdf-ocr-ai de mani√®re s√ªre."""
        # 1. Chercher dans le PATH syst√®me
        executable = shutil.which("pdf-ocr-ai")
        if executable:
            return Path(executable)
        
        # 2. Chercher dans le site-packages
        import sysconfig
        scripts_dir = Path(sysconfig.get_path("scripts"))
        candidate = scripts_dir / "pdf-ocr-ai"
        if sys.platform == "win32":
            candidate = candidate.with_suffix(".exe")
        
        if candidate.exists():
            return candidate
        
        raise ProcessingError(
            "pdf-ocr-ai non trouv√©. Installez-le avec: pip install pdf-ocr-ai"
        )

    def process(self, file_path: Path, config: RAGConfig) -> Path:
        output_path = config.processed_dir / (file_path.stem + ".md")
        pdf_ocr_cmd = self._find_executable()
        
        # Validation des chemins
        if not file_path.exists():
            raise ProcessingError(f"Fichier PDF introuvable: {file_path}")
        
        command = [
            str(pdf_ocr_cmd), str(file_path), str(output_path),
            "--provider", "ollama", "--model", config.pdf_model
        ]
        
        import subprocess
        try:
            result = subprocess.run(
                command,
                check=True,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace',
                timeout=3600  # 1h max
            )
            if result.stderr:
                logger.warning(f"pdf-ocr-ai warnings: {result.stderr}")
            return output_path
        except subprocess.TimeoutExpired:
            raise ProcessingError(f"Timeout lors du traitement de {file_path.name}")
        except subprocess.CalledProcessError as e:
            raise ProcessingError(f"pdf-ocr-ai a √©chou√©: {e.stderr}")
```

## 4. Optimisation du RAG avec Cache et Streaming

**Probl√®me** : Chargement r√©p√©t√© des documents pour BM25 et pas de streaming progressif.

**Solution** : Cache et streaming am√©lior√© :

```python
# src/rag_ollama/rag.py
from functools import lru_cache
import chromadb

@lru_cache(maxsize=1)
def get_bm25_retriever(vector_db: Chroma):
    """Cache le BM25 retriever (lourd √† construire)."""
    logger.info("Construction du BM25Retriever (mis en cache)...")
    all_docs = vector_db.get(include=["documents"])['documents']
    bm25 = BM25Retriever.from_texts(all_docs)
    bm25.k = 3
    return bm25

def setup_rag_chain(vector_db: Chroma, config: RAGConfig):
    logger.info(f"Configuration du mod√®le Ollama: {config.llm_model}...")
    llm = Ollama(model=config.llm_model)
    
    # Prompt optimis√© pour le fran√ßais
    prompt = ChatPromptTemplate.from_template("""Vous √™tes un assistant expert. R√©pondez √† la question en vous basant UNIQUEMENT sur le contexte fourni. Si vous ne trouvez pas la r√©ponse, dites clairement que vous ne savez pas.

Contexte: 
{context}

Question: {input}

R√©ponse (en fran√ßais):""")
    
    document_chain = create_stuff_documents_chain(llm, prompt)
    vector_retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    bm25_retriever = get_bm25_retriever(vector_db)
    
    return document_chain, EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever], 
        weights=[0.5, 0.5]
    )

# Am√©lioration du streaming
while True:
    question = input("\nVotre question: ")
    if question.lower() in ['exit', 'quit', 'q']: break
    
    try:
        relevant_docs = retriever.invoke(question)
        print("\nü§ñ R√©ponse: ", end="", flush=True)
        
        # Streaming avec gestion d'erreurs
        full_response = ""
        for chunk in doc_chain.stream({"input": question, "context": relevant_docs}):
            if chunk:
                print(chunk, end="", flush=True)
                full_response += chunk
        
        # Log la r√©ponse compl√®te
        logger.debug(f"R√©ponse g√©n√©r√©e: {full_response[:200]}...")
        print()
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration: {e}")
        print(f"\n‚ùå Erreur: {e}")
```

## 5. Benchmarking Robuste avec Gestion d'Erreurs

**Probl√®me** : Chemins en dur, pas de gestion de la m√©moire GPU.

**Solution** :

```python
# benchmark_models.py (am√©lior√©)
import torch  # Pour v√©rifier la m√©moire GPU
import humanize  # Pour affichage lisible

def check_gpu_memory() -> float:
    """Retourne la m√©moire GPU disponible en Go."""
    if torch.cuda.is_available():
        return torch.cuda.get_device_properties(0).total_memory / (1024**3)
    return 0.0

def run_benchmark(test_dir: Path):
    if not test_dir.exists():
        raise FileNotFoundError(f"Dossier de test introuvable: {test_dir}")
    
    # V√©rifier m√©moire disponible
    gpu_mem = check_gpu_memory()
    if gpu_mem > 0:
        logger.info(f"M√©moire GPU d√©tect√©e: {gpu_mem:.1f} Go")
    
    # ... suite du code ...
    
    for model in MODELS_TO_TEST:
        # Estimer la taille du mod√®le
        model_size_gb = estimate_model_size(model)  # √Ä impl√©menter
        
        if gpu_mem > 0 and model_size_gb > gpu_mem * 0.9:
            logger.warning(f"Mod√®le {model} trop grand pour la m√©moire GPU ({model_size_gb:.1f} Go > {gpu_mem:.1f} Go)")
            continue
        
        # ... processus de benchmark avec timeout ...
```

## 6. Tests Compl√©t√©s et Fiabilis√©s

**Probl√®me** : `test_rag_automated.py` d√©pend d'un environnement local non reproductible.

**Solution** : Utilisez des fixtures temporaires :

```python
# tests/test_rag_automated.py
import pytest
from pathlib import Path
import tempfile
import shutil

@pytest.fixture(scope="module")
def test_environment(tmp_path_factory):
    """Cr√©e un environnement de test complet avec des documents factices."""
    base_dir = tmp_path_factory.mktemp("rag_test")
    
    # Cr√©er des documents de test
    processed_dir = base_dir / "processed_md"
    processed_dir.mkdir()
    
    (processed_dir / "test_doc.md").write_text("""
    # Test Document
    
    Ceci est un document de test avec des informations sp√©cifiques.
    Le montant minimum du salaire impos√© est 1600 euros.
    """)
    
    db_dir = base_dir / "chroma_db"
    
    yield {
        "processed_dir": processed_dir,
        "db_dir": db_dir,
    }
    
    # Nettoyage automatique par pytest

def test_end_to_end_rag(test_environment):
    """Test E2E complet."""
    config = RAGConfig(
        processed_dir=test_environment["processed_dir"],
        db_path=test_environment["db_dir"],
    )
    
    # Indexation
    vector_db = load_or_initialize_vector_db(config)
    update_vector_db_incrementally(vector_db, config)
    
    # Requ√™te
    doc_chain, retriever = setup_rag_chain(vector_db, config)
    question = "Quel est le montant minimum du salaire impos√© ?"
    
    relevant_docs = retriever.invoke(question)
    response = doc_chain.invoke({"input": question, "context": relevant_docs})
    
    assert "1600" in response
```

## 7. Ajouts au `pyproject.toml`

**Probl√®mes** :
- `langchain_classic` n'existe pas (c'est `langchain`)
- `uv` ne g√®re pas les d√©pendances dev correctement avec `tool.uv`

**Solution** :

```toml
[project]
name = "rag-ollama"
version = "0.1.0"
description = "RAG system with Ollama, local embeddings, and AI-powered OCR"
readme = "README.md"
requires-python = ">=3.10"  # 3.13 est trop restrictif
dependencies = [
    "docling>=2.8,<3.0",
    "langchain>=0.2,<0.3",
    "langchain-community>=0.2,<0.3",
    "langchain-ollama>=0.2,<0.3",
    "langchain-chroma>=0.1,<0.2",
    "pydantic>=2.0,<3.0",
    "chromadb>=0.5,<0.6",
    "rank_bm25>=0.2,<0.3",
    "markdown>=3.5,<4.0",
    "requests>=2.31,<3.0",
    "ollama>=0.3,<0.4",
    "pdf-ocr-ai @ git+https://github.com/laurentvv/pdf-ocr-ai",
    "pyyaml>=6.0,<7.0",
    "tqdm>=4.66,<5.0",
    "humanize>=4.0,<5.0",  # Pour le benchmark
    "torch>=2.0",  # Pour d√©tection GPU
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0",
    "pytest-cov>=4.0",
    "black>=24.0",
    "ruff>=0.5",  # Remplacement moderne de flake8
]

[project.scripts]
rag-prepare = "rag_ollama.prepare_documents:main"
rag-chat = "rag_ollama.rag:main"
rag-add = "rag_ollama.add_document:main"
rag-benchmark = "rag_ollama.benchmark:main"  # Renomm√© pour plus de clart√©

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

## 8. Documentation README.md Am√©lior√©e

```markdown
## üîß Installation D√©veloppeur

```bash
# Cloner le repo
git clone https://github.com/laurentvv/rag-ollama
cd rag-ollama

# Installer avec uv (recommand√©)
uv sync --extra dev

# Configuration initiale
cp config.yaml.example config.yaml
# √âditer config.yaml selon vos besoins

# T√©l√©charger les mod√®les par d√©faut
make pull-models  # Ajoutez un Makefile avec les commandes ollama pull
```

## üìä Benchmark Personnalis√©

```bash
# Benchmark rapide avec vos propres documents
uv run rag-benchmark --dir "./mes_documents" --models qwen3-vl:8b llama3.2-vision

# Avec rapport HTML
uv run rag-benchmark --dir "./mes_documents" --output-format html --save-report
```

## üêõ D√©pannage

| Probl√®me | Solution |
|----------|----------|
| `OllamaError: Connection refused` | V√©rifiez qu'Ollama tourne: `ollama list` |
| `Model unavailable` | T√©l√©chargez le mod√®le: `ollama pull <nom>` |
| Timeout sur PDF | Augmentez `OPENAI_TIMEOUT` ou r√©duisez la taille du PDF |
| Erreurs d'encodage | Les fichiers sont sauvegard√©s en UTF-8 avec remplacement des caract√®res invalides |
```

## 9. Makefile pour Simplifier les T√¢ches Courantes

```makefile
# Makefile
.PHONY: install dev-install test benchmark pull-models clean

install:
	uv sync

dev-install:
	uv sync --extra dev

test:
	uv run pytest tests/ -v --cov=src/rag_ollama

benchmark:
	uv run python benchmark_models.py --dir "./test_docs"

pull-models:
	ollama pull gemma3:12b
	ollama pull embeddinggemma:latest
	ollama pull llama3.2-vision

clean:
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type d -name "*.egg-info" -exec rm -rf {} +
	rm -rf chroma_db/ processed_md/ benchmark_output/
```

## 10. Corrections de Bugs Critiques

### Bug 1 : `langchain_classic` inexistant
Dans `rag.py`, remplacez :
```python
# ‚ùå Mauvais imports
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.retrievers import EnsembleRetriever

# ‚úÖ Bons imports
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import EnsembleRetriever
```

### Bug 2 : `yaml_conf` non utilis√©
Dans tous les fichiers, utilisez la fonction `load_config()` mentionn√©e au point 1.

### Bug 3 : `benchmark_models.py` d√©pend de `prepare_documents`
Modifiez l'import pour qu'il fonctionne depuis n'importe o√π :

```python
# benchmark_models.py
import sys
from pathlib import Path

# Ajoute le src au path de mani√®re robuste
PROJECT_ROOT = Path(__file__).parent
sys.path.append(str(PROJECT_ROOT / "src"))

import rag_ollama.prepare_documents as prep_docs
```

---

## R√©sum√© des Actions Prioritaires

1. ‚úÖ **Impl√©menter le chargement de `config.yaml`** (copier le code du point 1)
2. ‚úÖ **Corriger les imports `langchain_classic` ** imm√©diatement
3. ‚úÖ ** Cr√©er `OllamaManager` ** pour centraliser les v√©rifications
4. ‚úÖ ** Renforcer la s√©curit√© des processeurs ** (validation des chemins)
5. ‚úÖ ** Ajouter `uv.lock` ** au `.gitignore` pour les projets uv
6. ‚úÖ ** Comparer les d√©pendances ** : `uv pip list` vs `pyproject.toml`

Ce refactoring rendra votre projet plus robuste, maintenable et pr√™t pour une utilisation en production. Les tests deviendront reproductibles sur n'importe quelle machine avec `pytest`.